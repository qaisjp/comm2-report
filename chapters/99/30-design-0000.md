\chapter{Design}

talk about progressive web applications:

- https://love2dev.com/blog/pwa-spa/
- https://developers.google.com/web/progressive-web-apps/

# Authentication

talk about passwords:

- https://www.chromium.org/developers/design-documents/create-amazing-password-forms
- https://www.chromium.org/developers/design-documents/form-styles-that-chromium-understands

# API Design

In [@sec:bg-api-analysis] we evaluated a number of existing APIs; based on that information we decided to adopt a REST architectural
style in designing our API. Additionally, the APIs in [@sec:bg-api-analysis] heavily inspired the design of our API.

TODO: why a progressive webapp?

Our progressive webapp uses its own _public_ API (rooted at `/v1/`) wherever possible (this is known as "dogfooding"), but we predict that
certain pages may take a while to load if multiple requests are involved. One example of such a page would be a user's profile, where
one request would be `/v1/users?username=qaisjp&exact=1` to find the user by username and get their user information and
then `/v1/users/1/resources` to list their resources.

In response to whether a website should use its own public API:

- Evers says that one "should not be updating the API frequently", that they should "spend the time to architect and proof out an API that will stay around for a while" and that "dogfooding in this way will enforce [this]" [-@eversShouldWebsiteUse2012].
- Dante says "Unless the performance overhead of using the web service is an issue, you should definitely use your public API. This will help you get a consistent behavior between your application and the consumers. It will also avoid code duplication [...]" [@danteArchitectureHowMy2013]

Based on this, we have chosen to also make use of _private_ internal endpoints. This will allow us to
"make data access more performant by using the database directly instead of doing extra requests" [@virkkunenApiDesignIt2010].
These internal endpoints live at `/private/` instead of the regular `/v1/` prefix, discouraging those that reverse engineer our webapp from building
software using these endpoints.

<!-- other links: TODO
- https://softwareengineering.stackexchange.com/questions/332864/fully-api-based-website-is-it-a-good-idea -->

## Permission management

We decided that permissions will be granular for site administrators,
but kept simple on a resource level. Original creators of a resource retain
permanent access rights to a resource and can also designate additional resource
administrators. These designated resource admins have all the same
permissions as the creator.

TODO: why?

## Deletions

A common approach to deleting entities in webapps is
to set an `is_deleted` flag to `true` and simply hide the row from
output.

This is useful if we need to maintain an audit log or (considering that
deletions are destructive) would like to undo changes.

Users, however, may not appreciate a website holding onto data they've
requested to be deleted.

When deleting _resources_ we can just run an SQL query like `delete from resources where resource_id = ?`
and let PostgreSQL cascade this via foreign keys.

Doing this for users raises the following questions:

1. When the user requests a deletion, their comments should be anonymised instead of outright removed (todo: why?)
2. When an admin deletes the account, the admin should decide whether or not their comments should be removed too. This allows admins to easily purge content from spammers.
3. What if an admin  post-comment-retaining-deletion, that
    their posts should have indeed been deleted?

(TODO) Deletions could:

- For comments and bans, allow `author_id` to be nullable. Show `[deleted]` in place.
    - This means we lose track of who the author was.
    - Reddit does this.
- When deleting user accounts, delete all the associated data, but keep the user row, and set `is_deleted` to true. We should also remove all personal data.
    - We also need to litter `if user.is_deleted then` everywhere

This is in line with the General Data Protection Regulation (GDPR) as only personally identifiable information needs to be removed. Comments do not contain sensitive data, and do not fall under the GDPR.

## Upload packages

A user should be able to send multiple `POST /v1/resources/:resource_id/pkg` requests, each creating a blank "draft".

TODO / QUESTION: which bits here are design and which bits are implementation? I suppose in Design we can do a more high level overview and then go into the nitty gritty stuff later in Implementation?

\textcolor{blue}{DESIGN... =} Once a file is uploaded by the client and is memory, we check the MIME
type of the uploaded file. If the file is not of the "application/zip"
MIME type, we return a "415 Unsupported Media Type" and discard the
data.

TODO: then we check the ZIP in memory using a number heuristics. What are these heuristics? Considering first getting somewhat feature parity first (or explaining why certain parts won't get implemented). And then add extra features in a new chapter after it's implemented?

\textcolor{blue}{MORE DESIGN... =} Once we've verified that the zip is safe to use, we upload to a storage
service using the `gocloud.dev/blob` library (a Go package).

> Blobs are a common abstraction for storing unstructured data on
> Cloud storage services and accessing them via HTTP. This guide shows
> how to work with blobs in the Go CDK. TODO: cite https://gocloud.dev/howto/blob

This is useful as it is a generic backend for various file storage
services, including support for Google Cloud Storage, Amazon S3, Azure
Blob Service, and of course Local Storage. This makes the website
scalable and resilient as we can rely on one of those services doing
backups for us, and also use them to deliver stuff for us. Less
bandwidth. But we can still use Local Storage when sysadmins are testing
or if they would prefer to use local storage (if they do not want to pay
for an external service)

\textcolor{blue}{IMPL... =} This library gives us safety as it converts filenames to
something safe. This means that we don't have to worry about malicious
filenames when storing files locally.

The filename `../test` is stored as `..__0x2f__test`. This is
secure.

However, gocloud allows filenames to contain forward slashes (creating a
subfolder). Although we could ensure that our filename has no directory
separators, we chose to force filenames to be stored as `pkg$ID.zip`
(such as `pkg6.zip`).

This additionally means that we don't need to write code to delete old
packages when reuploading a file (during initial package creation,
when in draft state). We can just rely on this library replacing the
blob with the new file.

We only need to delete the files when packages are deleted!

When implementing this we tried to use `io.Copy` to copy from the input
file to the bucket writer, but we could not do this. So, from the advice
of
https://stackoverflow.com/questions/39791021/how-to-read-multiple-times-from-same-io-reader (CITEME todo)
we refactored our initial code to use `io.TeeReader`.

> TeeReader returns a Reader that writes to w what it reads from r. All
> reads from r performed through it are matched with corresponding
> writes to w. There is no internal buffering - the write must complete
> before the read completes. Any error encountered while writing is
> reported as a read error. TODO CITEME FROM https://golang.org/pkg/io/#TeeReader

We realised that `TeeReader` returns an `io.Reader` which does not implement
`io.ReaderAt`, so in the end we chose to use `ioutil.ReadAll` to read the
entire file into a byte slice (`[]bytes`). This means that we don't need to
use `io.Copy`, and can produce a `io.ReaderAt` for the `archive/zip` library using
`bytes.NewReader` — this function returns a `bytes.Reader` which _does__ implement
`io.ReaderAt`.
CITEME TODO: https://stackoverflow.com/questions/50539118/golang-unzip-response-body/50539327

\textcolor{blue}{DESIGN... =} Once an actual package has been uploaded the user can choose to publish it, changing the package from the "draft" state to the "pending_review" state.

## Error handling

In initial experiments all errors were returned to the user — even internal errors. This is considered "leaky" (TODO: be more formal, don't say "leaky". You can quote https://cwe.mitre.org/data/definitions/209.html.) The API should instead always return the `http.StatusInternalServerError` server code and use logging facilities to output the error.

TODO: is it OK to say "in initial experiments" during the design phase?

Services like Sentry can also be used to keep track of production errors.

## Selecting HTTP status codes

### 401 Unauthorized vs. 403 Forbidden

The status text for *401* is *"401 Unauthorized"*, despite it being for _authentication_ and not _authorisation_.
This means it's important to carefully selected status codes for certain scenarios.

-   401: being unauthenticated for a request that requires authentication
-   403: being authenticated but not authorised to perform an action

Okta [-@oktaAuthenticationVsAuthorization2018] describes authentication as
"the act of validating that users are who they claim to be",
and authorisation as "the process of giving the user permission to access a specific resource or function".

[@irvineUnderstanding403Forbidden2011] says:

> [..] Receiving a 401 response is the server telling you, “you aren’t authenticated--either not authenticated at all or authenticated incorrectly--but please reauthenticate and try again.” [..]
>
> In summary, a 401 Unauthorized response should be used for missing or bad authentication, and a 403 Forbidden response should be used afterwards, when the user is authenticated but isn’t authorized to perform the requested operation on the given resource. [..]

Sources: (TODO CITEME)

-   https://stackoverflow.com/questions/3297048/403-forbidden-vs-401-unauthorized-http-responses
-   https://httpstatuses.com/401
-   https://httpstatuses.com/403
-   https://tools.ietf.org/html/rfc2616#section-10.4.2
